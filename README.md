Of course. Here is a recreated `README.md` file that incorporates your existing content and adds the detailed execution steps, including the option to use an existing log bucket.

-----

# AWS ALB Log Analyzer üìà

## Overview

This project is a serverless log analyzer for AWS Application Load Balancer (ALB) access logs. It is designed to be deployed via Terraform and run on-demand. When invoked, it processes log files from a specified S3 location, performs a detailed analysis, and generates a human-readable summary report in a separate S3 bucket.

The core of the application is a Python script deployed as an AWS Lambda function, providing a cost-effective, scalable, and maintenance-free solution for gaining insights into your web traffic.

-----

## Analysis Features

The analyzer generates a report covering four key areas, giving you a comprehensive view of your application's performance, health, and traffic patterns.

#### 1\. üöÄ Performance & Latency Monitoring

This analysis focuses on how fast your application is responding to user requests, using the unique timing fields available in ALB logs.

  * **Average Target Response Time**: Calculates the average `target_processing_time` across all requests to measure the real-world latency of your backend application.
  * **Top 5 Slowest Endpoints**: Identifies the specific API endpoints or pages with the highest average `target_processing_time` so you can focus optimization efforts where they are needed most.
  * **Latency Bottleneck Analysis**: Provides a breakdown of the average `request_processing_time`, `target_processing_time`, and `response_processing_time` to help determine if latency is caused by the client's network, your application, or the response delivery.

#### 2\. üö¶ Error & Availability Analysis

This section helps you distinguish between load balancer issues and application errors to quickly diagnose problems.

  * **ELB vs. Target Error Breakdown**: Compares the `elb_status_code` and `target_status_code` fields to differentiate between issues at the load balancer level (e.g., no healthy targets) and errors generated by your application.
  * **Top Client Errors (4xx)**: Lists the most frequent URLs that resulted in a `4xx` status code, helping you find broken links or bad API calls.
  * **Top Server Errors (5xx)**: Lists the most frequent URLs that resulted in a `5xx` status code, highlighting critical application failures.

#### 3\. üåê Traffic & Audience Insights

This part of the report gives you an overview of who is accessing your application and what they are requesting.

  * **Requests by Target Group**: Aggregates request counts by `target_group_arn` to show traffic distribution across your different microservices or application components.
  * **Top 10 Visitors**: Identifies the top 10 client IP addresses making the most requests.
  * **Total Bandwidth Usage**: Calculates the total data transfer by summing the `sent_bytes` field across all requests.

#### 4\. üîí Security & Compliance

This analysis provides insights into the security posture of your client connections and the actions being taken at the edge.

  * **TLS Security Summary**: Provides a count of requests by the `ssl_protocol` used, allowing you to monitor for legacy protocols.
  * **Actions Executed Summary**: Tallies the actions performed by the load balancer, such as `waf`, `redirect`, or `authenticate`, to audit how many requests are being processed by security rules.

-----

## ‚òÅÔ∏è Cloud Architecture

This analyzer is designed as a modern, event-driven, serverless application. This approach is highly efficient, automatically scalable, and extremely cost-effective.

### Architectural Flow

The architecture is simple and robust:

**[Manual Invocation] ‚û°Ô∏è [AWS Lambda Function] ‚û°Ô∏è reads from [S3 Bucket for Logs] ‚û°Ô∏è writes to [S3 Bucket for Reports]**

1.  **Manual Invocation**: A user triggers the **AWS Lambda function** manually, providing the S3 path to the logs they want to analyze.
2.  **Serverless Processing**: The Lambda function runs the Python script, finds all log files in the specified path, downloads, and analyzes them.
3.  **Report Storage**: After processing, the Lambda function saves the text-based analysis report to a separate **S3 bucket**.

-----

## üöÄ End-to-End Deployment and Execution Guide

This guide will walk you through setting up the required cloud infrastructure, deploying the Lambda function, and running the analysis on your ALB logs.

### Prerequisites

Before you begin, ensure you have the following installed and configured on your machine:

  * **Git**: To clone the project repository.
  * **AWS CLI**: To create S3 buckets and interact with your AWS account.
  * **Terraform**: To build and manage the cloud infrastructure.

You must also have your AWS credentials configured for the AWS CLI and Terraform to use.

### Step 1: Setup & Configuration

1.  **Clone the GitHub Repository**:

    ```bash
    git clone https://github.com/shreyasduraphe/AWS-ELB-LOG-ANALYZER.git
    cd AWS-ELB-LOG-ANALYZER
    ```

2.  **Prepare S3 Buckets**:

      * **Source Bucket (for Logs)**: You can either use an existing S3 bucket where your ALB access logs are already being delivered, or create a new one.
          * **Option A (Use Existing Bucket)**: Simply note the name of your bucket.
          * **Option B (Create a New Bucket)**: Run the following command, making sure to choose a globally unique name.
            ```bash
            aws s3 mb s3://your-unique-source-bucket-name --region us-west-2
            ```
      * **Destination Bucket (for Reports)**: You must create a new bucket to store the analysis reports.
        ```bash
        aws s3 mb s3://your-unique-destination-bucket-name --region us-west-2
        ```

3.  **Configure Terraform**: Open the `main.tf` file and replace the three placeholder bucket names with the unique names you are using.

    *Update the IAM policy resources:*

    ```terraform
    # ... inside the aws_iam_policy resource ...
    Resource = "arn:aws:s3:::your-unique-source-bucket-name/*"
    # ...
    Resource = "arn:aws:s3:::your-unique-source-bucket-name"
    # ...
    Resource = "arn:aws:s3:::your-unique-destination-bucket-name/*"
    ```

    *Update the Lambda environment variable:*

    ```terraform
    # ... inside the aws_lambda_function resource ...
    environment {
      variables = {
        DESTINATION_BUCKET = "your-unique-destination-bucket-name"
      }
    }
    ```

### Step 2: Deploy with Terraform üèóÔ∏è

These commands will build the IAM role, policy, and Lambda function in your AWS account.

1.  **Initialize Terraform**: This command prepares your project directory.
    ```bash
    terraform init
    ```
2.  **Plan the Deployment**: This shows you a preview of all the AWS resources that will be created.
    ```bash
    terraform plan
    ```
3.  **Apply the Configuration**: This command builds the infrastructure. It will ask for confirmation before proceeding.
    ```bash
    terraform apply
    ```
    Type `yes` and press **Enter** to approve.

### Step 3: Run the Analyzer ‚öôÔ∏è

Now that the Lambda function is deployed, you can run it manually.

1.  Navigate to the **AWS Lambda Console**.

2.  Find and click on your function, named **ALB\_Log\_Analyzer**.

3.  Go to the **"Test"** tab.

4.  Create a new test event with the following JSON payload. **Update the `source_bucket` and `source_prefix`** to point to the folder of logs you want to analyze.

    *Example JSON Payload:*

    ```json
    {
      "source_bucket": "your-unique-source-bucket-name",
      "source_prefix": "AWSLogs/123456789012/elasticloadbalancing/us-west-2/2025/09/16/"
    }
    ```

5.  Click the **"Test"** button to run the function.

### Step 4: Verify the Report ‚úÖ

After the function executes successfully, navigate to your destination S3 bucket in the AWS console. You will find the analysis report inside a `reports/` folder.

### Step 5: Cleanup (Optional but Recommended)

To avoid ongoing costs, you can remove all the resources created by Terraform.

1.  **Destroy the Infrastructure**: This command will delete the Lambda function and the IAM role/policy.
    ```bash
    terraform destroy
    ```
2.  **Delete S3 Buckets**: You must manually empty and delete the S3 buckets.